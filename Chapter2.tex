%!TEX root = Manuscript.tex

\chapter{Literature review}
\label{chap:review}
\minitoc

\section{Local feature matching}
Local feature refers to a discriminative structure found in an image, such as a point, corner, blob, edge or image patch. It is often accompanied with a descriptor, which is a compact vector representing the local neighborhood.
\par
According to different data storage types, descriptors can be divided into two categories: floating-point and binary descriptors. The former is recorded in floating-point format, which has the advantage of being informative. It is widely used in various matching scenarios.
The latter is stored in binary type, which guarantees faster processing while demanding less memory. It is particularly suitable for real-time and/or smartphone applications.
Since our goal is to match multi-epoch images for high accuracy cartography, we are interested in floating-point descriptors rather than binary ones.
\par
According to whether machine learning techniques are applied, local features can be categorized as hand-crafted or learned. We subsequently elaborate on the two categories of approaches.
\subsection{Hand-crafted methods}
In the early stage, Moravec detects corner feature by measuring the sum-of-squared-differences (SSD) by applying a small shift in a number of directions to the patch around a candidate feature \cite{moravec1980obstacle}. Based on this, Harris computes an approximation to the second derivative of the SSD with respect to the shift \cite{harris1988combined}. Since both Moravec and Harris are sensitive to changes in image scale, algorithms invariant to scale and affine transformations based on Harris are presented \cite{mikolajczyk2004scale}. Other than corner feature, SIFT (Scale-invariant feature transform) \cite{lowe2004distinctive} detects blob feature in scale-space, which is an entire pipeline including detection and description. It uses a difference-of-Gaussian function to identify potential feature points that are invariant to scale and orientation. SIFT is a milestone among hand-crafted features, and is able to outperform machine learning alternatives when matching conditions are favorable. RootSIFT \cite{arandjelovic2012three} uses a square root (Hellinger) kernel instead of the standard Euclidean distance to measure the similarity between SIFT descriptors, which leads to a dramatic performance boost. Similar to SIFT, SURF \cite{bay2006surf} resorts to integral images and Haar filters to extract blob feature in a computationally efficient way. DAISY \cite{tola2009daisy} is a local image descriptor, which uses convolutions of gradients in specific directions with several Gaussian filters to make it very efficient to extract dense descriptors. KAZE \cite{alcantarilla2012kaze} is an algorithm that detects and describes multi-scale 2D feature in nonlinear scale spaces. AKAZE \cite{Alcantarilla13bmvc} is an accelerated version based on KAZE.
\subsection{Learned methods}
With the rise of machine learning, learned features have shown their feasibility in the image matching problem when enough ground truth data is available. 
FAST \cite{rosten2006machine} uses decision tree to speed up the process of finding corner feature. 
LIFT (Learned Invariant Feature Transform) \cite{yi2016lift} is a deep network architecture that implements a full pipeline including detection, orientation estimation and feature description. It is based on the previous work TILDE \cite{verdie2015tilde}, the method of \cite{moo2016learning} and DeepDesc \cite{simo2015discriminative}. 
Tian et al. introduce L2-Net~\cite{tian2017l2} to learn high performance descriptor in Euclidean space via the \ac{CNN}. 
Afterwards Mishchuk et al.~\cite{mishchuk2017working} introduce a compact descriptor named HardNet, by applying a novel loss to L2Net \cite{tian2017l2}. 
DELF \cite{noh2017DELF} is an attentive local feature descriptor based on \ac{CNN}, which works particularly well for illumination changes.
SuperPoint \cite{detone2018superpoint} is a self-supervised, fully-convolutional model that operates on full-sized images and jointly computes pixel-level feature point locations and associated descriptors in one forward pass. 
LF-Net \cite{ono2018lf} is a deep architecture that embeds the entire feature extraction pipeline, and can be trained end-to-end with just a collection of images. 
D2-Net \cite{dusmanu2019d2} is a single neural network that works as a \textit{dense} feature descriptor and a feature detector simultaneously, but their keypoints are less accurate compared to classical features since they are extracted on feature maps which have a resolution of 1/4 of the input image.
ASLFeat~\cite{luo2020aslfeat} improves shape-awareness and localization accuracy by applying light-weight yet effective modifications on improved D2-Net.
R2D2 \cite{revaud2019r2d2} is a \ac{CNN} architecture that learns \textit{dense} local descriptors (one for each pixel) as well as two associated repeatability and reliability confidence maps.
Contextdesc ~\cite{luo2019contextdesc} is a unified learning framework that leverages and aggregates the cross-modality contextual information.
D2D \cite{wiles2020d2d} allows \textit{dense} features to be modified based on the differences between the images by conditioning the feature maps on both images. 
Different than the aforementioned feature extraction methods, SuperGlue \cite{sarlin2020superglue} presents a new way of thinking about the feature matching problem. 
%Instead of learning better task-agnostic local features followed by simple matching heuristics and tricks, SuperGlue
%It learns the matching process from pre-existing local features 
It matches two sets of pre-existing local features by adopting a flexible context aggregation mechanism based on attention to jointly find matches and reject non-matchable points, leading to robust matching results even in challenging situations.
\par
Early learned methods (LIFT ~\cite{yi2016lift}, L2-Net~\cite{tian2017l2}, HardNet~\cite{mishchuk2017working}, DELF ~\cite{noh2017DELF}, SuperPoint ~\cite{detone2018superpoint}, LF-Net ~\cite{ono2018lf}) use only intermediate metrics (e.g., repeatability, matching score, mean matching accuracy, etc) to evaluate the matching performance. 
Even though they demonstrate better performance when compared to hand-crafted features on certain benchmarks, it does not necessarily imply a better performance in terms of subsequent processing steps. For example, in the context of \ac{SfM}, finding additional matches for image pairs where SIFT already provides enough matches does not necessarily result in more accurate or complete reconstructions \cite{schonberger2017comparative}.
Jin et al.~\cite{jin2020image} introduce a comprehensive benchmark for local features and robust estimation algorithms, focusing on the accuracy of the reconstructed camera pose as the primary metric. Using the new metric, SIFT ~\cite{lowe2004distinctive} and SuperGlue ~\cite{sarlin2020superglue} take the lead~\cite{imagematchingchallenge2020}.

\section{Robust matching}
The goal of robust matching is to tell apart true matches (i.e., inliers) from false matches (i.e., outliers), and eliminate the latter from further processing.
\par
Typically, an iterative sampling strategy based on RANSAC (Random Sample Consensus) \cite{fischler1981random} relying on some mathematical model, such as homography \cite{sonka2014image} or essential matrix \cite{sonka2014image} is carried out to remove outliers. 
This is an important issue which was often not given sufficient attention.
LMedS (Least Median of Squares) \cite{leroy1987robust} is a meaningful groundwork before RANSAC, which is also commonly used to replace RANSAC.
MLESAC (Maximum Likelihood SAC)  \cite{torr2000mlesac} adopts the same sampling strategy as RANSAC but chooses the solution that maximizes the likelihood instead of the number of inliers. PROSAC (Progressive Sample Consensus ) \cite{chum2005matching} chooses samples from progressively larger sets of top-ranked matches, which makes it significantly faster than RANSAC. DEGENSAC \cite{chum2005two} is an algorithm for epipolar geometry estimation unaffected by planar degeneracy. It is widely used in the 2020 image matching challenge \cite{imagematchingchallenge2020}.
USAC (Universal RANSAC) \cite{raguram2012usac} framework is a synthesis of the various optimizations and improvements that have been proposed to RANSAC.
GC-RANSAC (Graph-Cut RANSAC) \cite{barath2018graph} runs graph-cut algorithm in the local optimization step.
MAGSAC \cite{barath2019magsac} eliminates the need for a user-defined inlier-outlier threshold with marginalization.
\par
Various deep learning methods have also been developed to handle the erroneous matches.
DSAC (the differentiable counterpart of RANSAC) \cite{brachmann2017dsac} replaces the deterministic hypothesis selection by a probabilistic selection.
CNe (Context Networks) \cite{moo2018learning} trains deep networks in an end-to-end fashion to label the matches as inliers or outliers, known intrinsics are required as input, and a post-processing with RANSAC is often tasked. CNe was embedded into the framework of \cite{jin2020image} to remove outliers, paired with DEGENSAC, PyRANSAC (a variant of DEGENSAC by disabling the degeneracy check, introduced in \cite{jin2020image}) and MAGSAC. The results showed that with SIFT used to train CNe, about 80\% of the outliers were filtered out. Nearly all classical methods benefited from CNe, but not the learned ones. Jin et al. \cite{jin2020image} also state that RANSAC should be tuned to particular feature detector and descriptor, and specific settings should be selected for a particular RANSAC variant.
\par
In this research, we use RANSAC to estimate the 3D Helmert transformation between surfaces (i.e., \ac{DSM}s) calculated in different epochs. Compared to the classical essential/fundamental matrix filtering, with less data we impose stricter rules on the sets of points. Lastly, we eliminate the remaining false matches by looking at their cross-correlation.

\section{Pose estimation}
Pose estimation describes the intrinsic and extrinsic parameters of an image and is classically solved with the \ac{SfM} algorithms \cite{snavely2006photo,deseilligny2011apero,schonberger2016structure} based on local feature matches. 
%If the image pose should be known in a reference coordinate system, i.e., be georeferenced, a 7-parameter transformation followed by \textit{a posteriori} bundle block adjustment must be carried out.
The accuracy of matches plays an important role throughout the \ac{SfM} process, since small inaccuracies in their locations can result in large errors in the estimated poses. 
Good matches will lead to better results on image orientation, camera calibration and 3D reconstruction \cite{lindenberger2021pixel}, \cite{sarlin2021back}, \cite{truong2018second}.
%Sarlin et al. \cite{lindenberger2021pixel}, Lindenberger et al. \cite{lindenberger2021pixel} and Truong Giang et al. \cite{truong2018second} show.

\par
Unlike in modern images where the image coordinate system overlaps with the camera coordinate system, in historical images the overlap is not maintained due to the scanning procedure. To account for this, an additional 2D transformation is estimated in the \ac{SfM} procedure \cite{article}, which puts higher demands on the matches. %Inaccurate features 
\cite{giordano2018toward} demonstrates the importance of good matches in estimating the camera calibration and its great impact on the planimetric and altimetric accuracies of the resulted \ac{DSM}. 
Systematic errors expressed as \textit{dome} effect (i.e., a vertical doming of the surface) could appear in the \ac{DSM}s when camera models are poorly estimated {(i.e., inaccurately estimated lens distortion parameters)} \cite{wackrow2008minimising}, \cite{james2014mitigating}, which restricts the wider use of \ac{DSM}s. 
%It results from inaccurately estimated lens distortion parameters \cite{wackrow2008minimising}, \cite{james2014mitigating}.

%\cite{giordano2018toward} demonstrates the importance of the self-calibration of historical images and its impact on 3D accuracy. Poorly modelled intrinsic parameters result in the known \textit{dome}-like deformations that the authors eliminate with the help of automatic GCPs from exiting orthophotos. 


%The tie-point extraction step is the opening in the photogrammetric
%processing chain and therefore plays a key role in the quality of the subsequent image orientation,
%camera calibration and 3D reconstruction. Improving its precision will have an impact on the
%obtained 3D. In this research work we describe a method which aims at enhancing the accuracy of
%image orientation by adding a second iteration photogrammetric processing.

%\par 

%The uncertain internal geometry can be partially resolved by calibration. Research recent conducted at Loughborough University indicated the potential of consumer grade digital sensors to maintain their internal geometry but also identified residual systematic error surfaces, discernible in digital elevation models (DEM), which are caused by slightly inaccurately estimated lens distortion parameters.


%\par
%Unlike in modern images where the image coordinate system overlaps with the planimetric camera coordinate system, in historical images the overlap is not maintained due to the scanning procedure. To account for this, an additional 2D transformation is estimated in the course of the processing \cite{article}. \cite{giordano2018toward} demonstrates the importance of the self-calibration of historical images and its impact on 3D accuracy. Poorly modelled intrinsic parameters result in the known \textit{dome}-like deformations that the authors eliminate with the help of automatic GCPs from exiting orthophotos. 
%\par 
%Many cases reported in the literature calculate the image poses with \ac{SfM} using features matched exclusively within the same epoch \cite{nurminen2015automation,cardenal2006use,fox2008unlocking,walstra2004time}. In multi-epoch scenarios, the individual epochs should be defined in a common frame, be it the frame of a reference epoch or an absolute reference frame (i.e., a projection coordinate frame). Control points derived from recent orthophotos and DEM \cite{nurminen2015automation,ellis2006measuring,fox2008unlocking} or GPS survey \cite{micheletti2015application,walstra2004time,cardenal2006use} serve to transform the individual epochs to the common frame. Alternatively, a coarse flight plan may provide an approximate co-registration \cite{giordano2018toward}.
%\par  
%To increase relative accuracy between several epochs,  \cite{cardenal2006use,korpela2006geometrically,micheletti2015application} manually insert inter-epoch tie-points. \cite{giordano2018toward} extracts tie-points between analogue images and recent images based on the method of \cite{aubry2014painting}. \cite{feurer2018joining} joins multi-epoch images in a single \ac{SfM} block based on SIFT-like algorithm \cite{lowe2004distinctive,semyonov2011algorithms} by making the assumption that a sufficient number of feature points remain invariant across each time period. Identifying permanent points over a long time-span has not reached a maturity and research in this domain remains insignificant. Most of the mentioned approaches rely on manual effort, auxiliary input or limiting hypothesis.

\section{Historical image processing}
%\subsection{General processing pipeline}
%Archiving and geoprocessing of historical aerial images: current status in europe,official publication no 70. European Spatial Data Research~\cite{sebastien2019archiving}
%\subsection{Inter-epoch historical images alignment}
%When it comes to inter-epoch historical images, however, directly applying SIFT or SuperGlue often results in inferior results due to large radiometric differences.
\zll{Compared to modern digital images, historical images are accompanied with particular characteristics such as poor radiometric quality and deformation during scanning. Therefore, aligning multi-epoch historical images by directly applying \textit{state-of-the-art} feature matching methods often leads to unsatisfactory results. }
In Figure~\ref{MultiEpochImgPair} we showed an example where SIFT and SuperGlue failed to recover good matches on an inter-epoch image pair with drastic scene changes. It is understandable as (1) SIFT is not sufficiently invariant over time, while (2) SuperGlue is not invariant to rotations larger than 45$^\circ$ and it underperforms on larger images because it was presumably trained on small images.\\
Therefore, many previous researches bypassed the task of extracting inter-epoch matches by processing different epochs separately followed by an inter-epoch co-registration relying on \ac{GCP}s.
Between 10 and 169 \ac{GCP}s are required in \cite{pinto2019archived}, ~\cite{bozek2019analysis}, \cite{persia2020archival}, ~\cite{micheletti2015application} and \cite{molg2017structure}.
Manually measuring \ac{GCP}s are laboursome and tedious. Furthermore, it is difficult to find salient points that are stable over time.\\
Certain attempts were made to extract inter-epoch matches. Giordano et al.~\cite{giordano2018toward} extract feature matches between historical and recent images relying on HoG descriptors~\cite{dalal2005histograms}. The authors require flight plans as input, which are not commonly available as mentioned in Section \ref{chap:intro}. 
Feurer et al.~\cite{feurer2018joining} joins multi-epoch images in a single \ac{SfM} block based on SIFT-like algorithm by making the assumption that a sufficient number of feature points remain invariant across each time period. Their methods are widely used in the subsequent works~\cite{filhol2019time}, ~\cite{cook2019simple},~\cite{parente2021automated} and~\cite{blanch2021multi}. It remains questionable whether the method is capable of handling drastic scene changes.
%Zhang et al.~\cite{zhang2020guided} extract inter-epoch matches from SIFT-detected keypoints based on the hypothesis that points follow 2D and 3D spatial similarity model. This method works in simple cases with few scene changes. 
{Additionally, a stream of previous works focus on historical terrestrial images (~\cite{maiwald2021automatic}, ~\cite{beltrami20193d}, ~\cite{bevilacqua2019reconstruction}, ~\cite{maiwald2019generation}) and historical video recordings (~\cite{maiwald2019generation}). However, their algorithms are not suitable to the aerial case.}\\
%This work is an extention of~\cite{zhang2020guided}. Unlike in~\cite{zhang2020guided}, we introduce a rough co-registration between different epochs based on matching \ac{DSM}s with SuperGlue, and use it to guide a precise matching. Our rough co-registration is robust under extreme scene changes because (1) SuperGlue utilizes context to enhance feature descriptors and (2) \ac{DSM}s are generally stable over time. With the guidance of roughly co-registered orientations and \ac{DSM}s, both SIFT and SuperGlue achieved good performance, as shown in our experiments.
In this work, we propose a rough-to-precise strategy to recover inter-epoch matches, without requiring any \ac{GCP}s or auxiliary data.
